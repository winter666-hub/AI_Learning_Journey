{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d74aae1",
   "metadata": {},
   "source": [
    "# 04. Vectorization – Linear Regression\n",
    "\n",
    "## Definition\n",
    "\n",
    "Vectorization은 반복문 (for - loop)을 사용하지 않고  \n",
    "벡터/행렬 연산을 통해 한 번에 계산하는 방식이다.  \n",
    "\n",
    "\n",
    "수학적으로는 $f(x)=w^Tx+b$\n",
    "\n",
    "코드로는 np.dot(w, x) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369f6b23",
   "metadata": {},
   "source": [
    "# Why Vertorzation?\n",
    "\n",
    "벡터화는 단순히 코드를 짧게 만들기 위한 것이 아니다.\n",
    "\n",
    "1. 속도 향상\n",
    "    - 파이썬 for문은 느리다 (인터프리터 오버헤드)\n",
    "    - numpy는 내부적으로 C/BLAS/SIMD 최적화 사용\n",
    "\n",
    "2. 수학식과 코드의 일치\n",
    "    - 수학: $w^Tx$\n",
    "    - 코드: np.dot(w, x)\n",
    "    - 구조가 동일하다\n",
    "3. 대규모 학습 가능\n",
    "    - feature 수가 방대할 때 필수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b611e9b8",
   "metadata": {},
   "source": [
    "# Dot product 구현 비교\n",
    "\n",
    "## Loop 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccb0fa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c816995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_loop(w, x):\n",
    "    total = 0\n",
    "    for j in range(len(w)):\n",
    "        total += w[j] * x[j]\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89747309",
   "metadata": {},
   "source": [
    "## Vectorization 방식\n",
    "\n",
    "np.dot(w, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110234dd",
   "metadata": {},
   "source": [
    "# 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5e599ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop time: 0.030144691467285156\n",
      "Vectorized time: 0.0005080699920654297\n"
     ]
    }
   ],
   "source": [
    "n = 200000\n",
    "w = np.random.randn(n)\n",
    "x = np.random.randn(n)\n",
    "\n",
    "# Loop 방식\n",
    "start = time.time()\n",
    "dot_loop(w, x)\n",
    "end = time.time()\n",
    "print(\"Loop time:\", end - start)\n",
    "\n",
    "# Vertorized\n",
    "start = time.time()\n",
    "np.dot(w, x)\n",
    "end = time.time()\n",
    "print(\"Vectorized time:\", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8a7429",
   "metadata": {},
   "source": [
    "# Gradient Update 벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865ff46b",
   "metadata": {},
   "source": [
    "## Loop 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50d259ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for j in range(len(w)):\n",
    "#   w[j] = w[j] - alpha * d[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e782e45",
   "metadata": {},
   "source": [
    "# Mathematical Insight\n",
    "dot product는 다음과 같다\n",
    "$$\n",
    "w \\cdot x = \\sum_{j=1}^{n} w_j x_j\n",
    "$$\n",
    "\n",
    "이는 각 feature에 대한 가중합(Weighted sum) 이다.  \n",
    "두 벡터의 방향 정렬 정도를 나타낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211fdf67",
   "metadata": {},
   "source": [
    "# Observations\n",
    "\n",
    "- Loop와 Vectorized 결과는 동일하다.\n",
    "- Vectorized 방식이 훨씬 빠르다.\n",
    "- 수학식과 코드가 거의 동일하다.\n",
    "- 벡터화는 구현 최적화가 아니라, 선형대수 기반 ML의 필수 구조이다.\n",
    "\n",
    "Vectorization does not change the mathematics of linear regression.  \n",
    "It changes how we express the computation to match the hardware structure of modern computers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
